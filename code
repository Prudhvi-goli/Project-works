# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Data Import and Pre-processing
# Load the dataset
data_path = '/content/drive/MyDrive/data/MCI_2014_to_2017.csv'
df = pd.read_csv(data_path, sep=',')

# Print the actual column names in the DataFrame
print(df.columns)

# List of relevant columns for the model
# Ensure these column names exactly match the names in your DataFrame
columns_of_interest = ['occurrenceyear',   'occurrencemonth','occurrenceday','occurrencedayofyear','occurrencedayofweek','occurrencehour','MCI',   'Division', 'Hood_ID','premisetype']

# Filter the DataFrame to include only relevant columns
df_filtered = df[columns_of_interest]

# Factorize categorical variables
MCI_encoded = pd.factorize(df_filtered['MCI'])
df_filtered['MCI'] = MCI_encoded[0]  # Not strictly necessary, but keeps things consistent
MCI_definitions = MCI_encoded[1]  # This creates the missing MCI_definitions

premise_encoded = pd.factorize(df_filtered['premisetype'])
df_filtered['premisetype'] = premise_encoded[0]
premise_definitions = premise_encoded[1]

year_encoded = pd.factorize(df_filtered['occurrenceyear'])
df_filtered['occurrenceyear'] = year_encoded[0]
year_definitions = year_encoded[1]

month_encoded = pd.factorize(df_filtered['occurrencemonth'])
df_filtered['occurrencemonth'] = month_encoded[0]
month_definitions = month_encoded[1]

hood_encoded = pd.factorize(df_filtered['Hood_ID'])
df_filtered['Hood_ID'] = hood_encoded[0]
hood_definitions = hood_encoded[1]

hour_encoded = pd.factorize(df_filtered['occurrencehour'])
df_filtered['occurrencehour'] = hour_encoded[0]
hour_definitions = hour_encoded[1]

day_week_encoded = pd.factorize(df_filtered['occurrencedayofweek'])
df_filtered['occurrencedayofweek'] = day_week_encoded[0]
day_week_definitions = day_week_encoded[1]

day_year_encoded = pd.factorize(df_filtered['occurrencedayofyear'])
df_filtered['occurrencedayofyear'] = day_year_encoded[0]
day_year_definitions = day_year_encoded[1]

# One-Hot Encode 'Division' column
division_encoded = pd.get_dummies(df_filtered['Division'], prefix='Division') 
df_filtered = df_filtered.drop('Division', axis=1)  # Drop the original 'Division' column
df_filtered = pd.concat([df_filtered, division_encoded], axis=1)  # Add the encoded columns

# Prepare features and target variable
X = df_filtered.drop(['MCI'], axis=1).values
y = df_filtered['MCI'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=21)

# One-Hot Encoding for categorical variables
encoder = OneHotEncoder(sparse_output=False)
encoded_X = encoder.fit_transform(X)

# Split the encoded data into training and testing sets
X_train_OH, X_test_OH, y_train_OH, y_test_OH = train_test_split(encoded_X, y, test_size=0.25, random_state=21)

# Numeric Encoded Model with Random Forest
rf_classifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)
rf_classifier.fit(X_train, y_train)
y_pred_rf = rf_classifier.predict(X_test)

# Evaluate the Random Forest model
print("Random Forest Model Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf, target_names=MCI_definitions))

# One-Hot Encoded Model with Random Forest
rf_classifier_OH = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)
rf_classifier_OH.fit(X_train_OH, y_train_OH)
y_pred_rf_OH = rf_classifier_OH.predict(X_test_OH)

# Evaluate the One-Hot Encoded Random Forest model
print("One-Hot Encoded Random Forest Model Accuracy:", accuracy_score(y_test_OH, y_pred_rf_OH))
print("Confusion Matrix:\n", confusion_matrix(y_test_OH, y_pred_rf_OH))
print("Classification Report:\n", classification_report(y_test_OH, y_pred_rf_OH, target_names=MCI_definitions))

# Random Forest with Balanced Class Weight
rf_classifier_balanced = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42, class_weight='balanced')
rf_classifier_balanced.fit(X_train, y_train)
y_pred_balanced = rf_classifier_balanced.predict(X_test)

# Evaluate the balanced Random Forest model
print("Balanced Random Forest Model Accuracy:", accuracy_score(y_test, y_pred_balanced))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_balanced))
print("Classification Report:\n", classification_report(y_test, y_pred_balanced, target_names=MCI_definitions))

# Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10, random_state=42)
gb_classifier.fit(X_train_OH, y_train_OH)  # Fit the model on the one-hot encoded training data

# Make predictions on the one-hot encoded test data
y_pred_gb_OH = gb_classifier.predict(X_test_OH)

# Evaluate the Gradient Boosting model
print("Gradient Boosting Model Accuracy:", accuracy_score(y_test_OH, y_pred_gb_OH))
print("Confusion Matrix:\n", confusion_matrix(y_test_OH, y_pred_gb_OH))
print("Classification Report:\n", classification_report(y_test_OH, y_pred_gb_OH, target_names=MCI_definitions))

# Summary of results
print("Summary of Model Accuracies:")
print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}")
print(f"One-Hot Encoded Random Forest Accuracy: {accuracy_score(y_test_OH, y_pred_rf_OH)}")
print(f"Balanced Random Forest Accuracy: {accuracy_score(y_test, y_pred_balanced)}")
print(f"Gradient Boosting Accuracy: {accuracy_score(y_test_OH, y_pred_gb_OH)}")
